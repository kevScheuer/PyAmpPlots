{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Aggregate and Plot AmpTools .fit Results\n",
    "If you're here, you've most likely just finished running an amplitude analysis using the [AmpTools](https://github.com/mashephe/AmpTools) framework, and now you have a whole bunch of fit results, stored as `.fit` files, that you need to start plotting. Within this tutorial you will learn how to:\n",
    "1. Aggregate these fit and data files into flattened `.csv` files\n",
    "2. Load and plot the `.csv` fit results using python's pandas and matplotlib libraries   \n",
    "\n",
    "As mentioned in the [repo's README](../README.md), this tutorial assumes you are already familiar with how AmpTools functions and the basics of amplitude analysis. Please note that this tutorial will *not* cover plotting the fit's angular distributions. That would require the full information of the `.root` files and is effectively handled by the [halld_sim plotter scripts](https://github.com/JeffersonLab/halld_sim/blob/master/src/programs/AmplitudeAnalysis/vecps_plotter/vecps_plotter.cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "1. At the top right of the notebook your language / kernel is listed. Make sure `.venv (Python 3.9.18)` is selected. If the option does not appear, make sure to the virtual environment is active (see [**Setup > Python** in the README](../README.md) for details).\n",
    "2. Next we want to ensure that our GlueX environment is setup. Normally we could simply run `source setup_gluex.csh` if we were doing this in the terminal, but we'll need to go about it a special way to run this in the jupyter notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# first lets define the parent dir (repo home)\n",
    "parent_dir = str(Path().resolve().parents[0]) # 0 is the 1st parent directory, i.e. the repo home\n",
    "print(parent_dir)\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, parent_dir) # add the repo home directory to the list of directories Python uses to look for modules\n",
    "\n",
    "# run the source script (done here in csh, but bash could be done instead)\n",
    "command = f\"source {parent_dir}/setup_gluex.csh && env\"\n",
    "proc = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True, executable='/bin/csh')\n",
    "output, _ = proc.communicate()\n",
    "\n",
    "# Parse the environment variables\n",
    "env_vars = {}\n",
    "for line in output.decode().splitlines():\n",
    "    key, value = line.split('=', 1)\n",
    "    env_vars[key] = value\n",
    "\n",
    "# add the environment variables\n",
    "os.environ.update(env_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Aggregation\n",
    "We will want to create the following 2 files in preparation for our analysis:\n",
    "1. `data.csv`: constructed from the `.root` data files in each mass bin, containing the information for that bin\n",
    "2. `best_fits.csv`: contains all the fit results across the entire mass range, made from the \"best\" of all the randomized fits in each bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "If we want to plot our fit results, we need to include the original data we are actually fitting to. This information is unfortunately not included in the `.fit` files, so we need to read it into a `data.csv` file using [convert_to_csv.py](../scripts/convert_to_csv.py). These python scripts use `argparse`, so we can conveniently see its abilities through its help message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i $parent_dir/scripts/convert_to_csv.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now lets see what (sorted) files we are going to combine, and run the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i $parent_dir/scripts/convert_to_csv.py -i $parent_dir/data/*/*Amplitude.root -p\n",
    "%run -i $parent_dir/scripts/convert_to_csv.py -i $parent_dir/data/*/*Amplitude.root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now see a `data.csv` file here in the [analysis directory](./)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Results\n",
    "Now we will be combining all our `.fit` results across the mass bins into a flattened `.csv` file to prepare them for analysis via python. This is achieved again through the [convert_to_csv.py](../scripts/convert_to_csv.py) script, which behind the scenes interacts with [extract_fit_results.cc](../scripts/extract_fit_results.cc) to load the AmpTools `FitResults` class and import the information we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i $parent_dir/scripts/convert_to_csv.py -i $parent_dir/data/*/*best.fit -o best_fits.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "To ensure that our data is ready for analysis, lets load in our files using pandas dataframes and check them for any potential issues. First, we can simply print out the heads (first 5 rows) to get a sense of the structure. Note if you are working on VS Code and installed the suggested [`DataWrangler` extension]([ms-toolsai.datawrangler](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.datawrangler)) then you can open these dataframes directly from this notebook for easy viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load files\n",
    "df_fit = pd.read_csv(\"best_fits.csv\")\n",
    "df_data = pd.read_csv(\"data.csv\")\n",
    "\n",
    "df_fit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check that we have the same number of data files and fit files, and for any missing / NaN values:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_fit.shape[0] == df_data.shape[0])\n",
    "print(f\"Number of null values in the fit results: {df_fit.isnull().sum().sum()}\")\n",
    "print(f\"Number of null values in the data: {df_data.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we'll want to wrap our phase differences to all be within a $2\\pi$ range and converted from radians to degrees $(^\\circ)$. Lets wrap them within $(-\\pi,\\pi]$ using our helper functions in [utils](./utils.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import analysis.utils as utils\n",
    "utils.wrap_phases(df_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "With our data prepared, lets move on to analysis. To avoid having to edit these setting for every single figure, lets change the global defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams.update({\n",
    "    'figure.figsize': (10, 8),\n",
    "    'figure.dpi': 100,\n",
    "    'xtick.labelsize': 14,\n",
    "    'ytick.labelsize': 14,\n",
    "    'axes.labelsize': 16,\n",
    "    'legend.fontsize': 16,\n",
    "    'xtick.major.width': 2.0,\n",
    "    'ytick.major.width': 2.0,\n",
    "    'xtick.minor.width': 1.8,\n",
    "    'ytick.minor.width': 1.8,\n",
    "    'lines.markersize': 10,\n",
    "    'grid.alpha': 0.8,\n",
    "    'axes.grid': True\n",
    "})\n",
    "plt.minorticks_on()\n",
    "plt.close() # suppress outputting an empty plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also define a few common parameters to all plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_bins = df_data[\"m_center\"]\n",
    "bin_width = (df_data[\"m_high\"] - df_data[\"m_low\"])[0] # all bin widths are equal, so just use the first one\n",
    "coherent_sums = utils.get_coherent_sums(df_fit)\n",
    "phase_differences = utils.get_phase_differences(df_fit)\n",
    "\n",
    "for sum_type, sum_list in coherent_sums.items():\n",
    "    print(f\"{sum_type} -> {sum_list}\")\n",
    "for amp_pair, phase_column in phase_differences.items():\n",
    "    print(f\"{amp_pair} -> {phase_column}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mass independent fit results\n",
    "One of the simplest plots to make is the data we fit to and the fit result intensity in bins of mass, with the fit result decomposed into the coherent sum of our $J^P$ values $1^+$ and $1^-$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I prefer this colormap over the default\n",
    "jp_colors = matplotlib.colormaps[\"Dark2\"].colors\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# plot data as black dots\n",
    "ax.errorbar(\n",
    "    x=mass_bins, y=df_data[\"events\"], xerr=bin_width / 2.0 , yerr=df_data[\"events_err\"], \n",
    "    fmt=\"k.\",label=\"Signal MC\",\n",
    ")\n",
    "\n",
    "# Plot Fit Result as a grey histogram with its error bars\n",
    "ax.bar(\n",
    "    x=mass_bins, height=df_fit[\"detected_events\"], width=bin_width,        \n",
    "    color=\"0.1\", alpha=0.15, label=\"Fit Result\",\n",
    ")\n",
    "ax.errorbar(\n",
    "    x=mass_bins, y=df_fit[\"detected_events\"], yerr=df_fit[\"detected_events_err\"],\n",
    "    fmt=\",\", color=\"0.1\", alpha=0.2, markersize=0,\n",
    ")\n",
    "\n",
    "# plot 1+\n",
    "ax.errorbar(\n",
    "    x=mass_bins, y=df_fit[\"1p\"], xerr=bin_width/2.0, yerr=df_fit[\"1p_err\"], \n",
    "    label=utils.convert_amp_name(\"1p\"), # converts amplitude / phase differences to be in J^P L_m^(e) format\n",
    "    linestyle=\"\", # want only markers, no lines\\\n",
    "    marker=\"1\",\n",
    "    color=jp_colors[2]\n",
    ")\n",
    "\n",
    "# plot 1-\n",
    "ax.errorbar(\n",
    "    x=mass_bins, y=df_fit[\"1m\"], xerr=bin_width/2.0, yerr=df_fit[\"1m_err\"], \n",
    "    label=utils.convert_amp_name(\"1m\"), # converts amplitude / phase differences to be in J^P L_m^(e) format\n",
    "    linestyle=\"\", # want only markers, no lines\\\n",
    "    marker=\"s\",\n",
    "    color=jp_colors[3]\n",
    ")\n",
    "\n",
    "ax.set_xlabel(r\"$\\omega\\pi^0$ inv. mass $(GeV)$\", loc=\"right\")\n",
    "ax.set_ylabel(f\"Events / {bin_width:.3f} GeV\", loc=\"top\")\n",
    "ax.set_ylim(bottom=0.0)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the individual waves perform though? Lets view them in a grid of $m$-projections and $L$ angular momenta values, where each plot shares the reflectivity contributions. The reflectivities will be colored according to the unofficial official convention for $\\color{red}natural~(\\varepsilon=+1)$ and $\\color{blue}unnatural~(\\varepsilon=-1)$ exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some dictionaries to convert characters <-> integers\n",
    "char_to_int = {\"m\": -1, \"0\": 0, \"p\": +1, \"S\": 0, \"P\": 1, \"D\": 2}\n",
    "int_to_char = {-1: \"m\", 0: \"0\", +1: \"p\"}\n",
    "pm_dict = {\"m\": \"-\", \"p\": \"+\"}\n",
    "\n",
    "# sort the JPL values by the order of S, P, D, F waves, and the m-projections\n",
    "jpl_values = sorted(coherent_sums[\"JPL\"], key=lambda JPL: char_to_int[JPL[-1]])\n",
    "m_ints = sorted({char_to_int[JPmL[-2]] for JPmL in coherent_sums[\"JPmL\"]})\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    nrows=len(jpl_values),\n",
    "    ncols=len(m_ints),\n",
    "    sharex=True,    \n",
    "    #sharey=True, # uncomment to compare relative intensities\n",
    "    figsize=(15, 10),\n",
    ")\n",
    "\n",
    "# iterate through JPL (sorted like S, P, D, F wave) and sorted m-projections\n",
    "for row, jpl in enumerate(jpl_values):\n",
    "    for col, m in enumerate(m_ints):\n",
    "\n",
    "        # force sci notation so large ticklabels don't overlap with neighboring plots\n",
    "        axs[row,col].ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0,0))\n",
    "        \n",
    "        # recombine jpl and m to get string needed to access the column\n",
    "        JPmL = f\"{jpl[0:2]}{int_to_char[m]}{jpl[-1]}\"\n",
    "\n",
    "        # set the labels for the first rows and columns\n",
    "        if row == 0:\n",
    "            axs[row, col].set_title(f\"m={m}\", fontsize=18)\n",
    "        if col == 0:\n",
    "            J = JPmL[0]\n",
    "            P = pm_dict[JPmL[1]]\n",
    "            L = JPmL[-1]\n",
    "            axs[row, col].set_ylabel(rf\"${J}^{{{P}}}{L}$\", fontsize=18)\n",
    "\n",
    "        # plot the negative reflectivity contribution        \n",
    "        neg_plot = axs[row, col].errorbar(\n",
    "            x=mass_bins, y=df_fit[f\"m{JPmL}\"], xerr=bin_width/2.0, yerr=df_fit[f\"m{JPmL}_err\"],\n",
    "            marker=\"v\", linestyle=\"\", markersize=8,\n",
    "            color=\"blue\", \n",
    "            alpha=0.5, # prevent overlap from cluttering the view\n",
    "            label=r\"$\\varepsilon=-1$\",\n",
    "        )\n",
    "        # plot the positive reflectivity contribution\n",
    "        pos_plot = axs[row, col].errorbar(\n",
    "            x=mass_bins, y=df_fit[f\"p{JPmL}\"], xerr=bin_width/2.0, yerr=df_fit[f\"p{JPmL}_err\"],\n",
    "            marker=\"^\", linestyle=\"\", markersize=8,\n",
    "            color=\"red\",\n",
    "            alpha=0.5,\n",
    "            label=r\"$\\varepsilon=+1$\",\n",
    "        )\n",
    "\n",
    "# reset limits to 0 for all plots\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.set_ylim(bottom=0)\n",
    "\n",
    "# figure cosmetics\n",
    "fig.text(0.5, 0.04, r\"$\\omega\\pi^0$ inv. mass (GeV)\", ha=\"center\", fontsize=20)\n",
    "fig.text(\n",
    "    0.04, 0.5, f\"Events / {bin_width:.3f} GeV\", \n",
    "    ha=\"center\", va=\"center\", \n",
    "    rotation=\"vertical\", rotation_mode=\"anchor\", fontsize=20,\n",
    ")\n",
    "\n",
    "# the pos/neg_plot variables get overwritten in the loop, so we're only passing one set of handles to the figure,\n",
    "# which is okay since all plots have the same legend. If we didn't do this, every single plots redundant legend would be displayed\n",
    "fig.legend(handles=[pos_plot, neg_plot], loc=\"upper right\") # \n",
    "plt.show()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the phase differences? Lets take a look at our two dominant amplitudes from separate $J^P$ values and plot their corresponding phase difference together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, sharex=True, gridspec_kw={\"wspace\": 0.0, \"hspace\": 0.07}, height_ratios=[3, 1])\n",
    "\n",
    "amp1, amp2 = \"p1p0S\", \"p1mpP\"\n",
    "\n",
    "# plot the first amplitude\n",
    "axs[0].errorbar(\n",
    "    x=mass_bins, y=df_fit[amp1], xerr=bin_width/2.0, yerr=df_fit[f\"{amp1}_err\"],\n",
    "    marker=\"o\", linestyle=\"\", color=\"green\",\n",
    "    label=utils.convert_amp_name(amp1),\n",
    ")\n",
    "# plot the second amplitude\n",
    "axs[0].errorbar(\n",
    "    x=mass_bins, y=df_fit[amp2], xerr=bin_width/2.0, yerr=df_fit[f\"{amp2}_err\"],\n",
    "    marker=\"s\", linestyle=\"\", color=\"purple\",\n",
    "    label=utils.convert_amp_name(amp2),\n",
    ")\n",
    "\n",
    "# plot the phase difference. Since there is an inherent ambiguity in the sign of the phase difference within\n",
    "# the model, we need to plot both signs to accommodate for every possible phase motion\n",
    "phase_dif = phase_differences[(amp1, amp2)]\n",
    "axs[1].errorbar(\n",
    "    x=mass_bins, y=df_fit[phase_dif], xerr=bin_width/2.0, yerr=df_fit[f\"{phase_dif}_err\"].abs(), \n",
    "    marker=\".\", linestyle=\"\", color=\"black\"\n",
    ")\n",
    "axs[1].errorbar(\n",
    "    x=mass_bins, y=-df_fit[phase_dif], xerr=bin_width/2.0, yerr=df_fit[f\"{phase_dif}_err\"].abs(), \n",
    "    marker=\".\", linestyle=\"\", color=\"black\"\n",
    ")\n",
    "\n",
    "# cosmetics\n",
    "axs[0].set_ylim(bottom=0.0)\n",
    "axs[0].set_ylabel(f\"Events / {bin_width:.3f} GeV\", loc=\"top\")\n",
    "\n",
    "axs[1].set_yticks(np.linspace(-180, 180, 5))  # force to be in pi intervals\n",
    "axs[1].set_ylim([-180, 180])\n",
    "axs[1].set_ylabel(r\"Phase Diff. ($^{\\circ}$)\", loc=\"center\")\n",
    "axs[1].set_xlabel(r\"$\\omega\\pi^0$ inv. mass $(GeV)$\", loc=\"right\")\n",
    "\n",
    "axs[0].legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Examples\n",
    "For more complex implementations and analysis, check out my own [analysis at the neutralb1 repo on github](https://github.com/kevScheuer/neutralb1/tree/main/analysis). There you'll find a large [Plotter class](https://github.com/kevScheuer/neutralb1/blob/main/analysis/scripts/pwa_tools.py#L22), where much of this tutorial is adapted from, along with several notebooks demonstrating its usage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
